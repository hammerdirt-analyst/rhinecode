{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys, file and nav packages:\n",
    "import os\n",
    "import datetime as dt\n",
    "import csv\n",
    "\n",
    "# math packages:\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime as dt \n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "# from scipy.stats import gamma\n",
    "# from scipy.stats import beta\n",
    "# from scipy.stats import binom\n",
    "# from scipy.stats import bernoulli\n",
    "\n",
    "# import statsmodels.api as sm\n",
    "# from statsmodels.graphics.gofplots import qqplot_2samples\n",
    "# from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "# charting:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import ticker\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "# from matplotlib.gridspec import GridSpec\n",
    "# from matplotlib import cm\n",
    "# from matplotlib.colors import ListedColormap, LinearSegmentedColormap, Colormap\n",
    "\n",
    "# mapping\n",
    "# import geopandas as gpd\n",
    "\n",
    "# home brew utitilties\n",
    "import resources.utilities.utility_functions as ut\n",
    "import resources.utilities.abundance_classes as ac\n",
    "\n",
    "# documenting\n",
    "from IPython.display import display, Markdown, Latex, HTML\n",
    "\n",
    "\n",
    "# returns the p_value for each test\n",
    "def kendall_pval(x,y):\n",
    "    return kendalltau(x,y)[1]\n",
    "\n",
    "def pearsonr_pval(x,y):\n",
    "    return pearsonr(x,y)[1]\n",
    "\n",
    "def spearmanr_pval(x,y):\n",
    "    return spearmanr(x,y)[1]\n",
    "\n",
    "# table kwargs\n",
    "table_k = dict(loc=\"top left\", bbox=(0,0,1,1), colWidths=[.5, .5], cellLoc='center')\n",
    "tablecenter_k = dict(loc=\"top left\", bbox=(0,0,1,1), cellLoc='center')\n",
    "tabtickp_k = dict(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "# chart kwargs\n",
    "title_k = {'loc':'left', 'pad':14, 'linespacing':1.5, 'fontsize':12}\n",
    "title_k14 = {'loc':'left', 'pad':16, 'linespacing':1.5, 'fontsize':14}\n",
    "xlab_k = {'labelpad':10, 'fontsize':12}\n",
    "ylab_k = {'labelpad':14, 'fontsize':14}\n",
    "\n",
    "\n",
    "# # use these to format date axis in charts\n",
    "# weeks = mdates.WeekdayLocator(byweekday=1, interval=4)\n",
    "# # onedayweek = mdates.DayLocator(bymonthday=1, interval=1)\n",
    "# # everytwoweeks = mdates.WeekdayLocator(byweekday=1, interval=4)\n",
    "\n",
    "# months = mdates.MonthLocator(bymonth=[3,6,9,12])\n",
    "# bimonthly = mdates.MonthLocator(bymonth=[1,3,5,7,9,11])\n",
    "# allmonths = mdates.MonthLocator()\n",
    "# wks_fmt = mdates.DateFormatter('%d')\n",
    "# mths_fmt = mdates.DateFormatter('%b')\n",
    "\n",
    "# map marker size:\n",
    "survey_data, location_data, code_defs, geo_data, output = ut.make_local_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "\n",
    "Based on the \n",
    "#### geographic spread\n",
    "\n",
    "While \n",
    "#### abundance - regional differences\n",
    "\n",
    "After tobacco objects, Polystrene pieces are ~13% of the total objects found, followed by plastic sheeting ~5%, and waste water ~5%. The most numerous objects are different for each water feature. For example on Lac Léman, Brienzersee and Lago Maggiore polystyrene and food products replace tobacco products as the the most numerous objects __\\(figure 3, 4 and 5\\)__.\n",
    "\n",
    "#### influences\n",
    "\n",
    "results suggest \n",
    "\n",
    "#### just cigarette butts\n",
    "\n",
    "Considered individually\n",
    "### Conclusion\n",
    "\n",
    "Considered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters:\n",
    "start_date = '2020-04-01'\n",
    "end_date = '2021-04-01'\n",
    "\n",
    "startyearmonth = '{}/{}'.format(start_date[5:7], start_date[:4])\n",
    "endyearmonth = '{}/{}'.format(end_date[5:7], end_date[:4]) \n",
    "\n",
    "# decide which data to use\n",
    "aggregated = False\n",
    "\n",
    "\n",
    "# collect the names:\n",
    "# group_names = list(these_groups.keys())\n",
    "\n",
    "# choose a lake:\n",
    "# lake = 'Lac Léman'\n",
    "# coi = 'Neuchâtel'\n",
    "# bassin_label = 'Aare'\n",
    "# bassin = ['Aare', 'Aare|Nidau-Büren-Kanal','Schüss', 'Neuenburgersee', 'Thunersee','Bielersee', 'Brienzersee','La Thièle']\n",
    "# bassin_lmn = ['Rhône', 'Lac Léman']\n",
    "# samples_all = 'All samples'\n",
    "\n",
    "\n",
    "\n",
    "# define a significant event:\n",
    "sig = .9\n",
    "one_minus_sig = (1-sig)\n",
    "\n",
    "# define explanatory variables:\n",
    "expv = ['population','streets','buildings','rivs']\n",
    "\n",
    "# name the folder:\n",
    "name_of_project = 'setup_class_methods'\n",
    "\n",
    "# use this to store things:\n",
    "project_directory = ut.make_project_folder(output, name_of_project)\n",
    "\n",
    "# keep track of output\n",
    "files_generated = []\n",
    "figure_num = 0\n",
    "data_num = 0\n",
    "\n",
    "def add_output(**kwargs):\n",
    "    files_generated.append({'tag':kwargs['tag'], 'number':kwargs['figure_num'], 'file':kwargs['file'],'type':kwargs['a_type']})\n",
    "    if kwargs['a_type'] == 'data':\n",
    "        kwargs['data'].to_csv(F\"{kwargs['file']}.csv\", index=False)\n",
    "    else:\n",
    "        plt.savefig(F\"{kwargs['file']}.jpeg\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "dutch_surveys = pd.read_csv(F\"{output}/harmonized_results/dutch_harmonized.csv\")\n",
    "\n",
    "dutch_beaches = pd.read_csv(F\"{output}/harmonized_results/dutch_admin_h.csv\")\n",
    "\n",
    "dutch_codes = pd.read_csv(F\"{code_defs}/new_dutch_codes.csv\")\n",
    "\n",
    "\n",
    "# the dutch codes set the start and end\n",
    "start_date = dutch_surveys['date'].min()\n",
    "end_date = dutch_surveys['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the aggregated hd data. This accounts for the custom codes used in Switzerland\n",
    "swiss_surveys = pd.read_csv(survey_data+'/aggregated_hd_surveys.csv')\n",
    "\n",
    "# these are the water features that drain to the Rhine:\n",
    "water_names = ['Aare', 'Sihl', 'Reuss', 'Rhein', 'Limmat', 'Seez','Zurichsee','Quatre Cantons', 'Vorderrhein', 'Zugersee','Glatt', 'Goldach', 'Greifensee','Bodensee', 'Chriesbach', 'Emme', 'Neuenburgersee', 'Walensee' ]\n",
    "\n",
    "# format the date and slice the data \n",
    "swiss_surveys['date'] = pd.to_datetime(swiss_surveys['date'], format='%Y-%m-%d')\n",
    "swissDf = swiss_surveys[(swiss_surveys.date >= start_date)&(swiss_surveys.date <= end_date)].copy()\n",
    "\n",
    "# location data\n",
    "swiss_beaches = pd.read_csv(location_data+'/hammerdirt_beaches.csv')\n",
    "\n",
    "# code data from switzerland\n",
    "swiss_codes = pd.read_csv(F\"{code_defs}/swiss_codes_keyed_ospar.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmonize data results for class methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unused columns, change column names to match the swiss beaches\n",
    "dutch_beaches.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_beaches.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_beaches.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_beaches.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_beaches = dutch_beaches.drop(['Unnamed: 0', 'Coordinaten','meting'], axis=1)\n",
    "dutch_beaches.rename(columns={'name':'location', 'Rivier':'water_name'}, inplace=True)\n",
    "dutch_beaches['country'] = 'NL'\n",
    "\n",
    "# fix the swiss data\n",
    "swiss_beaches = swiss_beaches.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\n",
    "\n",
    "print(dutch_beaches.columns)\n",
    "print(swiss_beaches.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_surveys['type'] = 'r'\n",
    "dutch_surveys['date'] = pd.to_datetime(dutch_surveys.date, format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_beaches.set_index('slug', inplace=True)\n",
    "\n",
    "# assign the water feature type to each result\n",
    "\n",
    "# make maps\n",
    "lakes = swiss_beaches.loc[swiss_beaches.water == 'l'].index\n",
    "rivers = swiss_beaches.loc[swiss_beaches.water == 'r'].index\n",
    "\n",
    "# apply maps:\n",
    "swissDf.loc[swissDf.location.isin(lakes), 'type'] = 'l'\n",
    "swissDf.loc[swissDf.location.isin(rivers), 'type'] = 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key in the ospar id to the swiss survyes\n",
    "swiss_codes.set_index('code', inplace=True)\n",
    "\n",
    "# map that value\n",
    "os_mlw_map = swiss_codes.ospar_id\n",
    "\n",
    "# apply map\n",
    "swissDf['ospar_id'] = swissDf.code.map(lambda x: os_mlw_map.loc[x] )\n",
    "\n",
    "# swap the code values\n",
    "swiss_surveys = swissDf[swissDf.water_name.isin(water_names)].drop('code', axis=1)\n",
    "swiss_surveys.rename(columns={'ospar_id':'code'}, inplace=True)\n",
    "\n",
    "# identify the swiss surveys as a group\n",
    "swiss_surveys['region'] = 'CH'\n",
    "swiss_surveys.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_surveys.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key the ospar code value into the dutch surveys\n",
    "current_codes = dutch_surveys.code.unique()\n",
    "\n",
    "# make a column to store the new code\n",
    "dutch_surveys['new_code'] = 0\n",
    "\n",
    "# make a map\n",
    "d_c_map = dutch_codes[['ID', 'parent_code']]\n",
    "d_c_map.set_index('ID', drop=True, inplace=True)\n",
    "\n",
    "# apply map\n",
    "for a_code in current_codes:\n",
    "    dutch_surveys.loc[dutch_surveys.code==a_code, 'new_code']= d_c_map.loc[a_code][0]\n",
    "\n",
    "# swap out the columns\n",
    "dutch_surveys.drop('code', inplace=True, axis=1)\n",
    "dutch_surveys.rename(columns={'new_code':'code'}, inplace=True)\n",
    "\n",
    "# identify the surveys as a whole\n",
    "dutch_surveys['region'] ='NL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_all = pd.concat([swiss_surveys, dutch_surveys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_all.groupby('region').quantity.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(swiss_surveys.quantity.sum(), dutch_surveys.quantity.sum())\n",
    "\n",
    "s_samps = swiss_surveys.loc_date.nunique()\n",
    "d_samps = dutch_surveys.loc_date.nunique()\n",
    "print(s_samps, d_samps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_all.groupby('region').loc_date.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the columns that the class method expects to see:\n",
    "these_cols = ['loc_date', 'location', 'water_name','type', 'date', 'region']\n",
    "\n",
    "a = ac.PreprocessData(surveys_all, dutch_beaches,these_cols=these_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_cols = ['region','water_name','type','loc_date', 'location', 'date']\n",
    "\n",
    "clas_kwargs = dict(\n",
    "#     code_group_data=group_names_locations,\n",
    "#     new_code_group=frag_plas,\n",
    "#     levels=levels,\n",
    "#     catchment_features=bassin,\n",
    "    end_date=end_date,\n",
    "    start_date=start_date,\n",
    "#     code_group_loc=output,\n",
    "    catchment_cols=catchment_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ac.CatchmentArea(a.processed, swiss_beaches, **clas_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic spread and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wfoams = ['G81', 'G82', 'G117']\n",
    "# yfoams = ['G74', 'G73', 'G75']\n",
    "# pand = ['G900', 'G901', 'G902']\n",
    "# candy = ['G30']\n",
    "# personal_hygiene = dfCodes.loc[dfCodes.source == 'Personal hygiene'].index.to_numpy()\n",
    "# tobacco_codes = dfCodes.loc[dfCodes.source == 'Tobacco'].index.to_numpy()\n",
    "\n",
    "# make a copy of the data to work with\n",
    "som_data = b.bassin_data[b.bassin_data.code != 117].copy()\n",
    "data_num+=1\n",
    "# 'tag':kwargs['tag'], 'number':kwargs['figure_num'], 'file':kwargs['file'],'type':kwargs['a_type']\n",
    "a_tag = 'data used for all calculations'\n",
    "\n",
    "file = F\"{project_directory}/all_data_used.csv\"\n",
    "a_type='data'\n",
    "\n",
    "add_output(tag=a_tag, figure_num=data_num, a_type=a_type, file=file, data=som_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = len(som_data.loc_date.unique())\n",
    "number_of_locations = len(som_data.location.unique())\n",
    "number_of_objects = som_data.quantity.sum()\n",
    "number_of_features = len(som_data.water_name.unique())\n",
    "print(F\"The number of samples, number of locations, the total number of objects and the number of cities:\\n\\n1. number of samples: {number_of_samples}\\n2. number of locations: {number_of_locations}\\n3. number of objects: {number_of_objects}\\n4. number of features: {number_of_features}\\n\")\n",
    "print(F\"\\nThe data is limited to all samples on or after {start_date} and up to {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F\"There are {len(som_data.water_name.unique())} different water features:\\n\\n{som_data.groupby('region').water_name.unique().to_numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of locations by feature\n",
    "num_loc_feature = som_data.groupby('water_name', as_index=False).location.nunique()\n",
    "print(F\"The number of sample locations by feature:\\n\\n{num_loc_feature.set_index('water_name')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som_data['fail'] = som_data.quantity > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dutch_top_ten = som_data[som_data.region == 'NL'].groupby(['code'], as_index=False).quantity.sum().sort_values(by='quantity',ascending=False)\n",
    "swiss_top_ten = som_data[som_data.region == 'CH'].groupby(['code'], as_index=False).quantity.sum().sort_values(by='quantity',ascending=False)\n",
    "# dutch_top_ten['% of total'] = dutch_top_ten.quantity/dutch_total\n",
    "swiss_top_ten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_top_ten[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtt_c = dutch_top_ten[:10].code.values\n",
    "dutch_total = dutch_top_ten.quantity.sum()\n",
    "dutch_top_ten['% of total'] = dutch_top_ten.quantity/dutch_total\n",
    "\n",
    "stt_c = swiss_top_ten[:10].code.values\n",
    "swiss_total = swiss_top_ten.quantity.sum()\n",
    "swiss_top_ten['% of total'] = swiss_top_ten.quantity/dutch_total\n",
    "\n",
    "# the top ten for each region\n",
    "coi_d = dutch_top_ten[dutch_top_ten.code.isin(dtt_c)]\n",
    "coi_s = swiss_top_ten[swiss_top_ten.code.isin(stt_c)]\n",
    "\n",
    "# the combined top ten\n",
    "\n",
    "combined = list(set(stt_c)|set(dtt_c))\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coi_all = som_data[som_data.code.isin(combined)].copy()\n",
    "\n",
    "coi_at = coi_all.groupby(['region', 'code'], as_index=False).agg({'quantity':'sum', 'pcs_m':'mean', 'fail':'sum', 'loc_date':'count'})\n",
    "coi_at.columns = coi_at.columns.get_level_values(0)\n",
    "coi_at['% of total'] = coi_at.quantity/coi_at.quantity.sum()\n",
    "coi_at['% found'] = coi_at.fail/coi_at.loc_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coi_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_codes[dutch_codes.parent_code.isin([46, 21,98, 4, 48, 89, 67, 93, 6, 19])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map_data = coi_at[['region', 'code', '% of total']].pivot(index='code', columns='region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,8))\n",
    "sns.heatmap(data=heat_map_data, cmap='rocket_r', ax=ax)\n",
    "ax.tick_params(axis='y', labelrotation=0)\n",
    "ax.set_ylabel(\"Ospar id\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title(\"% of total by region: combined top ten\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map_data = coi_at[['region', 'code', 'pcs_m']].pivot(index='code', columns='region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,8))\n",
    "sns.heatmap(data=heat_map_data, cmap='rocket_r', ax=ax)\n",
    "ax.tick_params(axis='y', labelrotation=0)\n",
    "ax.set_ylabel(\"Ospar id\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title(\"pieces per meter, combined top ten\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map_data = coi_at[['region', 'code', '% found']].pivot(index='code', columns='region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,8))\n",
    "sns.heatmap(data=heat_map_data, cmap='rocket_r', ax=ax)\n",
    "ax.tick_params(axis='y', labelrotation=0)\n",
    "ax.set_ylabel(\"Ospar id\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title(\"% of surveys found at least one\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "per_total_feature = pd.DataFrame(index=num_loc_feature.index, columns=[num_loc_feature.columns[1:]])\n",
    "for feature in per_total_feature.index:\n",
    "    for col in num_loc_feature.columns[1:]:\n",
    "        per_total_feature.loc[feature][col] = num_loc_feature.loc[feature][col]/num_loc_feature.loc[feature].location\n",
    "        \n",
    "per_total_feature.columns = per_total_feature.columns.get_level_values(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abundance: number of objects and percent of total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The files generated by this workbook:\\n\")\n",
    "files_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hopefully that just worked for you\n",
    "\n",
    "if not contact analyst@hammerdirt.ch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
